#### kafka rocketMq对比
```
kafka和Rocketmq都是分布式的消息系统，在集群化部署方面，kafka通过zk进行节点协调，而rocketmq通过自身namesrv进行节点协调，所以在协调节点的设计上rocket显得更加轻。
存储方面，
  在Kafka中，
    是1个topic（就是一个业务场景）有多个partition（对应3个物理文件目录），当需要存储数据的时候，会把topic中一个parition大文件分成多个小文件段。
    通过多个小文件段，就轻松实现定期清除或删除已经消费完文件。降低磁盘占用。
  在rocketmq中，
    采用的是混合型的存储结构，即为Broker单个实例下所有的队列共用一个日志数据文件（即为CommitLog）来存储。
  两者在对比上，Kafka采用的是独立型的存储结构，每个队列一个文件。
    RocketMQ采用混合型存储结构的缺点在于，会存在较多的随机读操作，因此读的效率偏低。同时消费消息需要依赖ConsumeQueue，构建该逻辑消费队列需要一定开销。 
    主从备份方面：
      生产者向kafka写入消息时，一般会写入多个分区（partition），kafka提供冗余机制，即每个分区都有多个相同的备份(replica)，
        kafka把分区所有副本均匀分配到其他broker上，并从这些副本中挑选一个作为leader副本对外提供服务，其他副本称为follwer副本。
        当leader副本所在的broker有可能宕机，这时follower副本会竞争成为leader，继续提供服务。 
      而当生产者者向rocketmq写入消息时，会将数据写入集群中的相关master broker上，而每个master broker都有1到多个slave broker,
        这样在一定程度上保证master出现了不可恢复的故障时，不丢失数据。
        同时如果master宕机了，消费者会自动重连到相应的salve上，不会出现消费停滞，
        那么同时在master和slave数据同步分为同步复制(有一定的效率损失)和异步复制(数据不一致) 
   在生产和消费消息方面: 
    在kafka中，每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic，物理上不同Topic的消息分开存储，
    逻辑上一个Topic的消息虽然保存于一个或多个broker上但只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处，而这种能力实现的底层我想应该就是通过zk来完成的。
    在rocketmq中，NameSrv提供
```
#### 客户端如何获取broker信息
 ```
 客户端主动去nameServer拉（定时发送请求去nameServer拉）
 ```
 #### NameServer与Broker互动（30s心跳机制 120s故障感知）
 ```
 broker -30s-> 向NameServer 报告自己活着
 naneS  -10s-> 检测broker最后一次心跳（间隔大于120s认为挂了）
  ```
 
#### 客户端如何感知Broker状态(是不是挂了)？？？
```
```

#### rocketMQ 主从 broker消息读写
```
写走主
读先走主，根据主的负载情况，给出建议是否走从(下次)

```
#### 数据分片理解
```
 也叫：Topic分片
一个Topic可以分布在各个Broker上，我们可以把一个Topic分布在一个Broker上的子集定义为一个Topic分片

```
#### 读写队列概念
 ```
 
 读写队列，则是在做路由信息时使用。在消息发送时，使用写队列个数返回路由信息，而消息消费时按照读队列个数返回路由信息。在物理文件层面，只有写队列才会创建文件。举个例子：写队列个数是8，设置的读队列个数是4.这个时候，会创建8个文件夹，代表0 1 2 3 4 5 6 7，但在消息消费时，路由信息只返回4，在具体拉取消息时，就只会消费0 1 2 3这4个队列中的消息，4 5 6 7中的信息压根就不会被消费。反过来，如果写队列个数是4，读队列个数是8，在生产消息时只会往0 1 2 3中生产消息，消费消息时则会从0 1 2 3 4 5 6 7所有的队列中消费，当然 4 5 6 7中压根就没有消息 ，假设消费group有两个消费者，事实上只有第一个消费者在真正的消费消息(0 1 2 3)，第二个消费者压根就消费不到消息。

由此可见，只有readQueueNums>=writeQueueNums,程序才能正常进行。最佳实践是readQueueNums=writeQueueNums。那rocketmq为什么要区分读写队列呢？直接强制readQueueNums=writeQueueNums，不就没有问题了吗？

rocketmq设置读写队列数的目的在于方便队列的缩容和扩容。思考一个问题，一个topic在每个broker上创建了128个队列，现在需要将队列缩容到64个，
   怎么做才能100%不会丢失消息，并且无需重启应用程序？

最佳实践：先缩容写队列128->64，写队列由0 1 2 ......127缩至 0 1 2 ........63。等到64 65 66......127中的消息全部消费完后，再缩容读队列128->64.(同时缩容写队列和读队列可能会导致部分消息未被消费)

我的理解：128->64假设读写比例是1:1，前64读，后64写，缩容后：前32写，后32读；假设同时缩容，读写 32:32，那么原来写队列 64中的后32个队列就无法被消费了，
  因为读队列只有32只能消费前32个 队列中的内容

 ```
